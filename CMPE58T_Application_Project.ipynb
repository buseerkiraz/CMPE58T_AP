{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/8tcnYmc1EszcBu/RRmDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buseerkiraz/CMPE58T_AP/blob/main/CMPE58T_Application_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This project is for CMPE58T (Advanced Natural Language Processing) Master's course in Boğaziçi\n",
        "University. It investigates methods to improve Turkish-to-English neural machine translation\n",
        "(NMT) by incorporating various types of linguistic and stylistic guidance into a baseline\n",
        "MarianMT model.\n",
        "\n",
        "The project is structured around several enhancements to a pre-trained baseline model\n",
        "(Helsinki-NLP/opus-mt-tr-en). First, a simple fine-tuning is performed on the WMT17 parallel\n",
        "corpus. Then, a morphological variant is developed by integrating rich morphological analyses\n",
        "(using the Sak et al. 2007 morphological parser), which are reformatted into a ConcatMorph\n",
        "structure to explicitly encode grammatical information in the input.\n",
        "\n",
        "A third direction involves training two separate GPT-2 based language models (LMs) — one on\n",
        "naturally-authored English sentences (from NewsCrawl) and another on machine-translated English\n",
        "sentences (from the GTNC corpus). These LMs are then used to compute naturalness scores for\n",
        "each sentence in the training set, allowing the model to tag them with either `<nat>` or\n",
        "`<trans>`. These tags are prepended to source sentences in order to fine-tune a NaturalMT\n",
        "model capable of producing more fluent and human-like translations.\n",
        "\n",
        "The models — Baseline, FinetunedMT, MorphMT, and NaturalMT — are compared both qualitatively\n",
        "(via direct translation examples) and quantitatively (using BLEU, chrF++, and BERTScore metrics).\n",
        "All training and evaluation outputs, including translations and metrics, are stored in Google\n",
        "Drive for reproducibility and analysis.\n",
        "\n",
        "Ultimately, the goal of this project is to explore how morphological awareness and stylistic\n",
        "fluency cues (naturalness tagging) can enhance the quality of machine translation systems\n",
        "for morphologically rich, low-resource languages like Turkish.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "k_h44HuyfZEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Go_tD2_eIap"
      },
      "outputs": [],
      "source": [
        "# Install a specific version of the Hugging Face Transformers library\n",
        "! pip install transformers==4.51.3 --quiet\n",
        "\n",
        "# Install additional libraries:\n",
        "# - datasets: for accessing and processing NLP datasets\n",
        "# - sacrebleu: for computing BLEU scores in a standardized way\n",
        "# - sentencepiece: for tokenization, commonly used in multilingual models\n",
        "# - accelerate: for optimized training and inference across devices\n",
        "# - evaluate: for evaluating model predictions\n",
        "! pip install datasets sacrebleu sentencepiece accelerate evaluate\n",
        "\n",
        "# Import the `load_dataset` function to load datasets from the Hugging Face Hub\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import the Dataset class to create or manipulate datasets in Hugging Face format\n",
        "from datasets import Dataset\n",
        "\n",
        "# Import PyTorch for model loading and tensor manipulation\n",
        "import torch\n",
        "\n",
        "# Import the `evaluate` module for evaluation metrics.\n",
        "import evaluate\n",
        "\n",
        "# Install the `bert_score` library for semantic evaluation using BERT embeddings\n",
        "! pip install bert_score\n",
        "\n",
        "# Import standard libraries for file handling and date-time processing\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE MODEL\n",
        "\n",
        "# Load the pre-trained MarianMT model and tokenizer for Turkish-to-English translation\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "baseline_model_name = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "baseline_model = MarianMTModel.from_pretrained(baseline_model_name)\n",
        "baseline_tokenizer = MarianTokenizer.from_pretrained(baseline_model_name)\n",
        "\n",
        "\n",
        "# Function to load parallel data (source and target files)\n",
        "# Returns a HuggingFace Dataset object with aligned sentence pairs\n",
        "def load_parallel_data(src_path, tgt_path, src_lang=\"tr\", tgt_lang=\"en\"):\n",
        "    with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
        "        src_lines = [line.strip() for line in f_src]\n",
        "        tgt_lines = [line.strip() for line in f_tgt]\n",
        "    data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines, tgt_lines)]\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Load WMT17 test set consisting of Turkish-English sentence pairs\n",
        "test_dataset = load_parallel_data(\"test.tr\", \"test.en\")\n",
        "\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model.to(device)  # Move model to the selected device\n",
        "baseline_model.eval()      # Set model to evaluation mode (no dropout etc.)\n",
        "\n",
        "# Load the evaluation metric for BLEU\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Extract source (TR) and reference (EN) sentences from the dataset\n",
        "sources = [ex[\"translation\"][\"tr\"] for ex in test_dataset]\n",
        "references = [ex[\"translation\"][\"en\"] for ex in test_dataset]\n",
        "\n",
        "# Placeholder list to store model predictions\n",
        "baseline_predictions = []\n",
        "batch_size = 16  # Define batch size for inference\n",
        "\n",
        "# Perform batch inference using beam search decoding\n",
        "for i in range(0, len(sources), batch_size):\n",
        "    batch_sources = sources[i:i+batch_size]\n",
        "    batch_refs = references[i:i+batch_size]\n",
        "\n",
        "    # Tokenize batch of source sentences\n",
        "    inputs = baseline_tokenizer(batch_sources, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move input tensors to device\n",
        "\n",
        "    # Generate translations without updating gradients\n",
        "    with torch.no_grad():\n",
        "        output_ids = baseline_model.generate(**inputs, max_length=128, num_beams=4)  # Beam search decoding\n",
        "        batch_preds = baseline_tokenizer.batch_decode(output_ids, skip_special_tokens=True)  # Convert token IDs to strings\n",
        "        baseline_predictions.extend(batch_preds)\n",
        "\n",
        "# Evaluate model predictions with BLEU, BERTScore, and ChrF++\n",
        "bleu = evaluate.load(\"sacrebleu\").compute(predictions=baseline_predictions, references=[[ref] for ref in references])\n",
        "bertscore_result = evaluate.load(\"bertscore\").compute(predictions=baseline_predictions, references=references, lang=\"en\")\n",
        "chrf_result = evaluate.load(\"chrf\").compute(predictions=baseline_predictions, references=references)\n",
        "\n",
        "\n",
        "# Mount Google Drive to save experiment outputs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a folder for saving baseline model outputs\n",
        "baseline_dir = \"/content/drive/MyDrive/mt_experiments/baseline\"\n",
        "os.makedirs(baseline_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "\n",
        "# Save translations as a JSONL file (line-delimited JSON for each example)\n",
        "translations_jsonl_path = os.path.join(baseline_dir, \"baseline-opus-mt-tr-en.translations.jsonl\")\n",
        "with open(translations_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for src, pred, ref in zip(sources, baseline_predictions, references):\n",
        "        line = {\"source\": src, \"prediction\": pred, \"reference\": ref}\n",
        "        f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "# Save evaluation metrics (BLEU, BERTScore, ChrF++) in a JSON file\n",
        "eval_result = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"BLEU\": bleu[\"score\"],\n",
        "    \"BERTScore\": {\n",
        "        \"precision\": sum(bertscore_result[\"precision\"]) / len(bertscore_result[\"precision\"]),\n",
        "        \"recall\": sum(bertscore_result[\"recall\"]) / len(bertscore_result[\"recall\"]),\n",
        "        \"f1\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n",
        "    },\n",
        "    \"ChrF++\": chrf_result[\"score\"]\n",
        "}\n",
        "\n",
        "eval_json_path = os.path.join(baseline_dir, \"baseline_evaluation_results.json\")\n",
        "with open(eval_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_result, f, indent=4)  # Save metrics in a readable format\n",
        "\n",
        "\n",
        "# Save predictions in a readable TXT file (source, prediction, reference)\n",
        "txt_path = os.path.join(baseline_dir, \"baseline_test_predictions.txt\")\n",
        "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(sources)):\n",
        "        f.write(f\"{i+1}.\\n\")\n",
        "        f.write(f\"SOURCE (TR):    {sources[i]}\\n\")\n",
        "        f.write(f\"BASELINE PRED:  {baseline_predictions[i]}\\n\")\n",
        "        f.write(f\"REFERENCE (EN): {references[i]}\\n\")\n",
        "        f.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "# Confirm that outputs have been saved\n",
        "print(\"baselineMT outputs saved to:\", baseline_dir)\n"
      ],
      "metadata": {
        "id": "XlhHxU2leOhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINETUNED MODEL\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to original WMT parallel corpus\n",
        "en_path = \"WMT-News.en-tr.en\"\n",
        "tr_path = \"WMT-News.en-tr.tr\"\n",
        "\n",
        "# Read English and Turkish lines from files\n",
        "with open(en_path, 'r', encoding='utf-8') as f_en, open(tr_path, 'r', encoding='utf-8') as f_tr:\n",
        "    en_lines = [line.strip() for line in f_en]\n",
        "    tr_lines = [line.strip() for line in f_tr]\n",
        "\n",
        "# Ensure both files have the same number of lines\n",
        "assert len(en_lines) == len(tr_lines), \"Line count mismatch...\"\n",
        "\n",
        "# Split the data into 80% train, 10% validation, 10% test\n",
        "train_en, temp_en, train_tr, temp_tr = train_test_split(en_lines, tr_lines, test_size=0.2, random_state=42)\n",
        "val_en, test_en, val_tr, test_tr = train_test_split(temp_en, temp_tr, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save split data to files\n",
        "def save_pairs(prefix, en, tr):\n",
        "    with open(f\"{prefix}.en\", \"w\", encoding=\"utf-8\") as f_en, open(f\"{prefix}.tr\", \"w\", encoding=\"utf-8\") as f_tr:\n",
        "        for e, t in zip(en, tr):\n",
        "            f_en.write(e + \"\\n\")\n",
        "            f_tr.write(t + \"\\n\")\n",
        "\n",
        "save_pairs(\"train\", train_en, train_tr)\n",
        "save_pairs(\"val\", val_en, val_tr)\n",
        "save_pairs(\"test\", test_en, test_tr)\n",
        "\n",
        "# Load dataset using HuggingFace's text loader (not yet tokenized or structured)\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"text\", data_files={\"en\": \"train.en\", \"tr\": \"train.tr\"})\n",
        "\n",
        "# OPTIONAL DEBUG: Show N lines of raw data\n",
        "with open(en_path, 'r', encoding='utf-8') as f_en, open(tr_path, 'r', encoding='utf-8') as f_tr:\n",
        "    for i in range(num_lines_to_show):  # variable must be defined elsewhere\n",
        "        en_line = f_en.readline().strip()\n",
        "        tr_line = f_tr.readline().strip()\n",
        "        print(f\"{i+1}. EN: {en_line}\")\n",
        "        print(f\"   TR: {tr_line}\\n\")\n",
        "\n",
        "# Utility to load parallel data into a HuggingFace-compatible dataset\n",
        "def load_parallel_data(src_path, tgt_path, src_lang=\"tr\", tgt_lang=\"en\"):\n",
        "    with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
        "        src_lines = [line.strip() for line in f_src]\n",
        "        tgt_lines = [line.strip() for line in f_tgt]\n",
        "    data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines, tgt_lines)]\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Load preprocessed training and test datasets\n",
        "train_dataset = load_parallel_data(\"train.tr\", \"train.en\")\n",
        "test_dataset = load_parallel_data(\"test.tr\", \"test.en\")\n",
        "\n",
        "# Load pretrained MarianMT model and tokenizer for Turkish-English\n",
        "model_name = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing function for tokenizing input-output sentence pairs\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"tr\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize training and validation data\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = val_dataset.map(preprocess_function, batched=True)  # val_dataset needs to be defined beforehand\n",
        "\n",
        "# Import training classes from Transformers\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments for fine-tuning\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./opus-mt-tr-en-finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",  # disables WandB or TensorBoard logging\n",
        ")\n",
        "\n",
        "# Define the Seq2SeqTrainer for training and evaluation\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"fine-tuned-opus-mt-tr-en\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-opus-mt-tr-en\")\n",
        "\n",
        "# Mount Google Drive to save the fine-tuned model there as well\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save model and tokenizer to Drive\n",
        "trainer.save_model(\"/content/drive/MyDrive/fine-tuned-opus-mt-tr-en\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/fine-tuned-opus-mt-tr-en\")\n",
        "\n",
        "# Reload test dataset for final evaluation\n",
        "def load_parallel_data(src_path, tgt_path, src_lang=\"tr\", tgt_lang=\"en\"):\n",
        "    with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
        "        src_lines = [line.strip() for line in f_src]\n",
        "        tgt_lines = [line.strip() for line in f_tgt]\n",
        "    data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines, tgt_lines)]\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "test_dataset = load_parallel_data(\"test.tr\", \"test.en\")\n",
        "\n",
        "# Setup for evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load evaluation metrics\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "chrf = evaluate.load(\"chrf\")\n",
        "\n",
        "# Extract source and reference sentences\n",
        "sources = [ex[\"translation\"][\"tr\"] for ex in test_dataset]\n",
        "references = [ex[\"translation\"][\"en\"] for ex in test_dataset]\n",
        "\n",
        "# Generate translations in batches\n",
        "predictions = []\n",
        "batch_size = 16\n",
        "\n",
        "for i in range(0, len(sources), batch_size):\n",
        "    batch_sources = sources[i:i+batch_size]\n",
        "    batch_refs = references[i:i+batch_size]\n",
        "\n",
        "    inputs = tokenizer(batch_sources, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_length=128, num_beams=4)\n",
        "        batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "        predictions.extend(batch_preds)\n",
        "\n",
        "# Compute and print BLEU score\n",
        "bleu = metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "print(f\"\\nTest BLEU score: {bleu['score']:.2f}\")\n",
        "\n",
        "# Compute BERTScore (semantic similarity)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "# Compute ChrF++ (character n-gram F-score)\n",
        "chrf_result = chrf.compute(predictions=predictions, references=references)\n",
        "print(f\"\\nChrF++ score: {chrf_result['score']:.2f}\")\n",
        "\n",
        "# Save all evaluation metrics to disk\n",
        "def save_eval_result(model_name, bleu, bertscore_result, chrf_result, save_path=\"evaluation_results.json\"):\n",
        "    new_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"BLEU\": bleu[\"score\"],\n",
        "        \"BERTScore\": {\n",
        "            \"precision\": sum(bertscore_result[\"precision\"]) / len(bertscore_result[\"precision\"]),\n",
        "            \"recall\": sum(bertscore_result[\"recall\"]) / len(bertscore_result[\"recall\"]),\n",
        "            \"f1\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n",
        "        },\n",
        "        \"ChrF++\": chrf_result[\"score\"]\n",
        "    }\n",
        "\n",
        "    # Append to existing file or create a new one\n",
        "    if os.path.exists(save_path):\n",
        "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            all_results = json.load(f)\n",
        "    else:\n",
        "        all_results = {}\n",
        "\n",
        "    all_results[model_name] = new_entry\n",
        "\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "\n",
        "    print(f\"Evaluation saved to '{model_name}'\")\n",
        "\n",
        "# Save evaluation scores to a JSON file\n",
        "save_eval_result(\n",
        "    model_name=\"fine-tuned-opus-mt-tr-en\",\n",
        "    bleu=bleu,\n",
        "    bertscore_result=bertscore_result,\n",
        "    chrf_result=chrf_result\n",
        ")\n",
        "\n",
        "# Save translations (source, reference, prediction) as JSONL\n",
        "def save_translations_jsonl(model_name, sources, references, predictions, filename=None):\n",
        "    if filename is None:\n",
        "        filename = f\"{model_name}.translations.jsonl\"\n",
        "\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for src, ref, pred in zip(sources, references, predictions):\n",
        "            json_line = {\n",
        "                \"source\": src,\n",
        "                \"reference\": ref,\n",
        "                \"prediction\": pred\n",
        "            }\n",
        "            f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"Saved translations to {filename}\")\n",
        "\n",
        "save_translations_jsonl(\"fine-tuned-opus-mt-tr-en\", sources, references, predictions)\n",
        "\n",
        "# Save evaluation scores again (redundant, but safe if done in separate runtime)\n",
        "save_eval_result(\n",
        "    model_name=\"fine-tuned-opus-mt-tr-en\",\n",
        "    bleu=bleu,\n",
        "    bertscore_result=bertscore_result,\n",
        "    chrf_result=chrf_result\n",
        ")"
      ],
      "metadata": {
        "id": "xEZIjDJmegRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MORPH MT\n",
        "\n",
        "# Download and unzip morphological parser files (Sak et al. (2007))\n",
        "!gdown --id 1rja1ejQPACYeRwGwb1TuwZJeUpiAfM4a\n",
        "!unzip Parser.zip\n",
        "\n",
        "# Install Python 2, which is required by the parser\n",
        "!apt-get install python2\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Define a simple tokenizer using regex to separate punctuation\n",
        "def basic_tokenize(text):\n",
        "    # Surround punctuation with spaces\n",
        "    text = re.sub(r\"([.,!?()\\\":;])\", r\" \\1 \", text)\n",
        "    # Collapse multiple spaces into one\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply the basic tokenizer to each line of test.tr and write to test.txt\n",
        "with open(\"test.tr\", encoding=\"utf-8\") as fin, open(\"test.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in fin:\n",
        "        if line.strip():\n",
        "            fout.write(basic_tokenize(line.strip()) + \"\\n\")\n",
        "\n",
        "# Function to run the morphological parser script in Python 2\n",
        "def run_script(filename):\n",
        "    os.system(f'python2.7 parse_corpus.py \"{filename}\" > \"morph_{filename}\"')\n",
        "\n",
        "# List of files to parse (only test.txt here)\n",
        "filenames = [\"test.txt\",]\n",
        "\n",
        "# Run the parser in parallel (for larger batches)\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    executor.map(run_script, filenames)\n",
        "\n",
        "# Disambiguate morphological analyses using provided Perl script and model\n",
        "!perl md.pl -disamb model.m \"morph_test.txt\" \"disamb_test.txt\"\n",
        "\n",
        "# Compress disambiguated output for backup\n",
        "!zip morph_files.zip disamb_test.txt*\n",
        "\n",
        "# Parse the output of the morphological disambiguator\n",
        "def parse_first_analysis(line):\n",
        "    \"\"\"\n",
        "    Extracts the first morphological analysis in word<lemma+TAG1+TAG2+...> format.\n",
        "    Returns 'word<unknown>' if analysis is missing or unknown.\n",
        "    \"\"\"\n",
        "    if line.startswith('<S') or line.startswith('</S') or not line.strip():\n",
        "        return None\n",
        "\n",
        "    parts = line.strip().split()\n",
        "    word = parts[0]\n",
        "\n",
        "    if '[Unknown]' in line:\n",
        "        return f\"{word}<unknown>\"\n",
        "\n",
        "    for token in parts[1:]:\n",
        "        if '[' not in token:\n",
        "            continue\n",
        "        analyses = re.findall(r\"([^+\\[\\]]*)\\[([^\\[\\]]+)\\]\", token)\n",
        "        if analyses:\n",
        "            lemma = analyses[0][0]\n",
        "            tags = [t for (_, t) in analyses]\n",
        "            tag_str = '+'.join(tags)\n",
        "            return f\"{word}<{lemma}+{tag_str}>\"\n",
        "\n",
        "    return f\"{word}<unknown>\"\n",
        "\n",
        "# Process full file of disambiguated morphological analyses\n",
        "def process_file(file_path, output_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('<S'):\n",
        "                sentence = []\n",
        "            elif line.startswith('</S'):\n",
        "                if sentence:\n",
        "                    sentences.append(' '.join(sentence))\n",
        "            else:\n",
        "                parsed = parse_first_analysis(line)\n",
        "                if parsed:\n",
        "                    sentence.append(parsed)\n",
        "    with open(output_path, 'w', encoding='utf-8') as out:\n",
        "        for s in sentences:\n",
        "            out.write(s + '\\n')\n",
        "\n",
        "# Convert morphological parser output into sentence format\n",
        "process_file(\"disamb_test.txt\", \"final_test.tr\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Convert parsed morphological forms into ConcatMorph format: gidiyorum_gitVerbProgPres1Sg\n",
        "def convert_to_concatmorph_format(tr_input_path, en_input_path, tr_output_path):\n",
        "    tr_lines = Path(tr_input_path).read_text(encoding='utf-8').splitlines()\n",
        "    en_lines = Path(en_input_path).read_text(encoding='utf-8').splitlines()\n",
        "\n",
        "    converted_lines = []\n",
        "\n",
        "    for line in tr_lines:\n",
        "        words = line.strip().split()\n",
        "        converted = []\n",
        "\n",
        "        for word in words:\n",
        "            if '<' in word and '>' in word:\n",
        "                try:\n",
        "                    base, morph = word.split('<', 1)\n",
        "                    morph = morph.rstrip('>')\n",
        "                    if '+Punc' in morph:\n",
        "                        converted_word = base  # exclude punctuation tags\n",
        "                    elif morph == \"unknown\":\n",
        "                        converted_word = base\n",
        "                    else:\n",
        "                        morph_concat = \"_\" + morph.replace(\"+\", \"\")\n",
        "                        converted_word = base + morph_concat\n",
        "                except ValueError:\n",
        "                    converted_word = word\n",
        "            else:\n",
        "                converted_word = word\n",
        "\n",
        "            converted.append(converted_word)\n",
        "\n",
        "        converted_lines.append(\" \".join(converted))\n",
        "\n",
        "    Path(tr_output_path).write_text(\"\\n\".join(converted_lines), encoding='utf-8')\n",
        "    print(f\"Converted {len(converted_lines)} lines to ConcatMorph format.\")\n",
        "    print(\"Turkish and English files are aligned.\")\n",
        "\n",
        "# Run ConcatMorph conversion\n",
        "convert_to_concatmorph_format(\"final_test.tr\", \"test.en\", \"test.concatmorph.tr\")\n",
        "\n",
        "# Load parallel dataset into Hugging Face Dataset object\n",
        "def load_parallel_data(src_path, tgt_path, src_lang=\"tr\", tgt_lang=\"en\"):\n",
        "    with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
        "        src_lines = [line.strip() for line in f_src]\n",
        "        tgt_lines = [line.strip() for line in f_tgt]\n",
        "    data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines, tgt_lines)]\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Load train/test datasets in ConcatMorph format\n",
        "train_dataset = load_parallel_data(\"train.concatmorph.tr\", \"train.en\")\n",
        "# val_dataset = load_parallel_data(\"final_val.tr\", \"val.en\")  # Optional\n",
        "test_dataset = load_parallel_data(\"test.concatmorph.tr\", \"test.en\")\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocess the tokenized dataset for Seq2Seq training\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"tr\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Define training arguments for the morph-aware model\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./opus-mt-tr-en-finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Train with Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained morph model locally\n",
        "trainer.save_model(\"morph-opus-mt2-tr-en\")\n",
        "tokenizer.save_pretrained(\"morph-opus-mt2-tr-en\")\n",
        "\n",
        "# Mount Google Drive to store trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the model to Google Drive\n",
        "trainer.save_model(\"/content/drive/MyDrive/morph-opus-mt2-tr-en\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/morph-opus-mt2-tr-en\")\n",
        "\n",
        "# Reload test set\n",
        "def load_parallel_data(src_path, tgt_path, src_lang=\"tr\", tgt_lang=\"en\"):\n",
        "    with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
        "        src_lines = [line.strip() for line in f_src]\n",
        "        tgt_lines = [line.strip() for line in f_tgt]\n",
        "    data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines, tgt_lines)]\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "test_dataset = load_parallel_data(\"test.concatmorph.tr\", \"test.en\")\n",
        "\n",
        "# Prepare model and device for evaluation\n",
        "sources = [ex[\"translation\"][\"tr\"] for ex in test_dataset]\n",
        "references = [ex[\"translation\"][\"en\"] for ex in test_dataset]\n",
        "\n",
        "morph_model_path = \"morph-opus-mt2-tr-en\"\n",
        "morph_tokenizer = MarianTokenizer.from_pretrained(morph_model_path)\n",
        "morph_model = MarianMTModel.from_pretrained(morph_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "morph_model.eval()\n",
        "\n",
        "morph_predictions = []\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 16\n",
        "\n",
        "# Run inference on morph-aware model\n",
        "for i in range(0, len(sources), batch_size):\n",
        "    batch_sources = sources[i:i+batch_size]\n",
        "\n",
        "    inputs = morph_tokenizer(batch_sources, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = morph_model.generate(**inputs, max_length=128, num_beams=4)\n",
        "        batch_preds = morph_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "        morph_predictions.extend(batch_preds)\n",
        "\n",
        "# Evaluate morph model\n",
        "bleu = evaluate.load(\"sacrebleu\").compute(predictions=morph_predictions, references=[[ref] for ref in references])\n",
        "bertscore_result = evaluate.load(\"bertscore\").compute(predictions=morph_predictions, references=references, lang=\"en\")\n",
        "chrf_result = evaluate.load(\"chrf\").compute(predictions=morph_predictions, references=references)\n",
        "\n",
        "# Save evaluation results and predictions to Drive\n",
        "morph_dir = \"/content/drive/MyDrive/mt_experiments/morph\"\n",
        "os.makedirs(morph_dir, exist_ok=True)\n",
        "\n",
        "# Save translations as JSONL\n",
        "translations_jsonl_path = os.path.join(morph_dir, \"morph-opus-mt2-tr-en.translations.jsonl\")\n",
        "with open(translations_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for src, pred, ref in zip(sources, morph_predictions, references):\n",
        "        line = {\"source\": src, \"prediction\": pred, \"reference\": ref}\n",
        "        f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Save evaluation metrics\n",
        "eval_result = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"BLEU\": bleu[\"score\"],\n",
        "    \"BERTScore\": {\n",
        "        \"precision\": sum(bertscore_result[\"precision\"]) / len(bertscore_result[\"precision\"]),\n",
        "        \"recall\": sum(bertscore_result[\"recall\"]) / len(bertscore_result[\"recall\"]),\n",
        "        \"f1\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n",
        "    },\n",
        "    \"ChrF++\": chrf_result[\"score\"]\n",
        "}\n",
        "\n",
        "eval_json_path = os.path.join(morph_dir, \"morph_evaluation_results.json\")\n",
        "with open(eval_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_result, f, indent=4)\n",
        "\n",
        "# Save readable prediction log\n",
        "txt_path = os.path.join(morph_dir, \"morph_test_predictions.txt\")\n",
        "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(sources)):\n",
        "        f.write(f\"{i+1}.\\n\")\n",
        "        f.write(f\"SOURCE (TR):    {sources[i]}\\n\")\n",
        "        f.write(f\"MORPH PRED:     {morph_predictions[i]}\\n\")\n",
        "        f.write(f\"REFERENCE (EN): {references[i]}\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"All morph outputs saved to\", morph_dir)"
      ],
      "metadata": {
        "id": "OZufdqp5ehkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NATURALMT: Train two LMs (natural & translationese), tag data, fine-tune NMT\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "import sys\n",
        "\n",
        "# Download and preprocess natural English monolingual corpus (NewsCrawl)\n",
        "!wget http://data.statmt.org/news-crawl/en/news.2020.en.shuffled.deduped.gz\n",
        "!gunzip news.2020.en.shuffled.deduped.gz\n",
        "!mkdir -p lm_data/natural\n",
        "\n",
        "# Filter lines by length and save 20K for training the natural LM\n",
        "!cat news.2020.en.shuffled.deduped \\\n",
        "  | awk 'length > 20 && length < 200' \\\n",
        "  | head -n 20000 > lm_data/natural/english.txt\n",
        "\n",
        "# Clone GTNC repository which contains Google Translations from NewsCrawl\n",
        "!git clone https://github.com/damiaanr/gtnc\n",
        "\n",
        "# Prepare translationese LM data by sampling lines from GTNC outputs\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "GTNC_TRANSLATED_DIR = Path(\"gtnc/output/translated\")\n",
        "OUTPUT_FILE = Path(\"lm_data/translationese/translationese_english.txt\")\n",
        "TARGET_LINE_COUNT = 20000\n",
        "LANGUAGES = [\"tr\", \"de\", \"zh\"]  # languages used to generate translationese\n",
        "\n",
        "def collect_selected_trg_files(directory, selected_langs):\n",
        "    files = []\n",
        "    for lang in selected_langs:\n",
        "        file_path = directory / f\"{lang}.trg\"\n",
        "        if file_path.exists():\n",
        "            files.append(file_path)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    return files\n",
        "\n",
        "def sample_sentences(files, max_lines=20000):\n",
        "    all_lines = []\n",
        "    for f in files:\n",
        "        with f.open(\"r\", encoding=\"utf-8\") as file:\n",
        "            lines = [line.strip() for line in file if line.strip()]\n",
        "            all_lines.extend(lines)\n",
        "    random.shuffle(all_lines)\n",
        "    return all_lines[:max_lines]\n",
        "\n",
        "def main():\n",
        "    os.makedirs(OUTPUT_FILE.parent, exist_ok=True)\n",
        "    trg_files = collect_selected_trg_files(GTNC_TRANSLATED_DIR, LANGUAGES)\n",
        "    print(f\"Using files: {[f.name for f in trg_files]}\")\n",
        "    sampled = sample_sentences(trg_files, TARGET_LINE_COUNT)\n",
        "    print(f\"Writing {len(sampled)} lines to {OUTPUT_FILE}...\")\n",
        "    with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as out:\n",
        "        for line in sampled:\n",
        "            out.write(line + \"\\n\")\n",
        "    print(\"done\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Disable WandB logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Setup and train GPT-2 based language model (either natural or translationese)\n",
        "from transformers import *\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_FILE = \"natural_english.txt\"  # set this to translationese_english.txt to train transLM\n",
        "OUTPUT_DIR = \"lm_nat_model\"\n",
        "EPOCHS = 3\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Load dataset for LM fine-tuning\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=DATA_FILE,\n",
        "    block_size=128,\n",
        "    overwrite_cache=True,\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=2,\n",
        "    eval_strategy=\"no\",\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Train language model\n",
        "trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset)\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model trained and saved in {OUTPUT_DIR}\")\n",
        "\n",
        "# Save trained LM to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "models = {\"lm_nat_model\": \"/content/drive/MyDrive/NMT_LMs/LM_Natural\"}\n",
        "for local_path, drive_path in models.items():\n",
        "    shutil.copytree(local_path, drive_path, dirs_exist_ok=True)\n",
        "\n",
        "print(\"Models saved to Google Drive\")\n",
        "\n",
        "# Load both LMs for scoring sentences\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths and settings\n",
        "NAT_MODEL_DIR = \"lm_nat_model\"\n",
        "TRANS_MODEL_DIR = \"lm_trans_model\"\n",
        "SOURCE_TR = \"train.tr\"\n",
        "TARGET_EN = \"train.en\"\n",
        "OUTPUT_TAGGED_TR = \"tagged.tr\"\n",
        "OUTPUT_TAGGED_EN = \"tagged.en\"\n",
        "THRESHOLD = 0.0\n",
        "\n",
        "# Load models and tokenizers\n",
        "tok_nat = GPT2Tokenizer.from_pretrained(NAT_MODEL_DIR)\n",
        "model_nat = GPT2LMHeadModel.from_pretrained(NAT_MODEL_DIR).eval()\n",
        "\n",
        "tok_trans = GPT2Tokenizer.from_pretrained(TRANS_MODEL_DIR)\n",
        "model_trans = GPT2LMHeadModel.from_pretrained(TRANS_MODEL_DIR).eval()\n",
        "\n",
        "def get_score(sentence, model, tokenizer):\n",
        "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs, labels=inputs)\n",
        "        return -outputs.loss.item()  # log-probability\n",
        "\n",
        "# Load training pairs\n",
        "with open(SOURCE_TR, encoding=\"utf-8\") as f_tr, open(TARGET_EN, encoding=\"utf-8\") as f_en:\n",
        "    src_lines = [line.strip() for line in f_tr]\n",
        "    tgt_lines = [line.strip() for line in f_en]\n",
        "\n",
        "assert len(src_lines) == len(tgt_lines), \"Mismatch between TR and EN lines!\"\n",
        "\n",
        "# Tag sentences based on naturalness score comparison\n",
        "tagged_src = []\n",
        "tagged_tgt = []\n",
        "for src, tgt in tqdm(zip(src_lines, tgt_lines), total=len(src_lines)):\n",
        "    p_nat = get_score(tgt, model_nat, tok_nat)\n",
        "    p_trans = get_score(tgt, model_trans, tok_trans)\n",
        "    tag = \"<nat>\" if (p_nat - p_trans) > THRESHOLD else \"<trans>\"\n",
        "    tagged_src.append(f\"{tag} {src}\")\n",
        "    tagged_tgt.append(tgt)\n",
        "\n",
        "# Save tagged training files\n",
        "Path(\"tagged.tr\").write_text(\"\\n\".join(tagged_src), encoding=\"utf-8\")\n",
        "Path(\"tagged.en\").write_text(\"\\n\".join(tagged_tgt), encoding=\"utf-8\")\n",
        "\n",
        "# Display some examples\n",
        "for i, line in enumerate(tagged_src[:20]):\n",
        "    print(f\"{i+1:>4}: {line}\")\n",
        "\n",
        "# Count tag distribution\n",
        "from collections import Counter\n",
        "tags = [line.split()[0] for line in tagged_src if line.strip().startswith(\"<\")]\n",
        "counts = Counter(tags)\n",
        "print(\"Tag counts in tagged.tr:\")\n",
        "print(f\"  <nat>   : {counts.get('<nat>', 0)}\")\n",
        "print(f\"  <trans> : {counts.get('<trans>', 0)}\")\n",
        "print(f\"  Total   : {sum(counts.values())}\")\n",
        "\n",
        "# Fine-tune NMT model on tagged dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "src_lines = Path(\"tagged.tr\").read_text(encoding=\"utf-8\").splitlines()\n",
        "tgt_lines = Path(\"tagged.en\").read_text(encoding=\"utf-8\").splitlines()\n",
        "assert len(src_lines) == len(tgt_lines)\n",
        "\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "OUTPUT_DIR = \"natural_nmt_model\"\n",
        "EPOCHS = 3\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = MarianMTModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Add <nat> and <trans> tokens to tokenizer\n",
        "special_tokens = [\"<nat>\", \"<trans>\"]\n",
        "tokenizer.add_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Convert to HF Dataset\n",
        "raw_data = {\"translation\": [{\"tr\": tr, \"en\": en} for tr, en in zip(src_lines, tgt_lines)]}\n",
        "dataset = Dataset.from_list(raw_data[\"translation\"])\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(example[\"tr\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"en\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"no\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save to disk\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"NMT model trained and saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# Save to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "shutil.copytree(\"natural_nmt_model\", \"/content/drive/MyDrive/NMT_LMs/Natural_NMT_Model\", dirs_exist_ok=True)\n",
        "\n",
        "# Inference on new sentences\n",
        "def translate_tr_sentences(sentences, style=\"<nat>\"):\n",
        "    inputs = [f\"{style} {s}\" for s in sentences]\n",
        "    encoded = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**encoded, max_length=128)\n",
        "    return [tokenizer.decode(g, skip_special_tokens=True) for g in outputs]\n",
        "\n",
        "# Sample WMT test sentences\n",
        "turkish_sentences = [...]\n",
        "translations = translate_tr_sentences(turkish_sentences)\n",
        "for i, (tr, en) in enumerate(zip(turkish_sentences, translations)):\n",
        "    print(f\"{i+1:>2}. TR: {tr}\")\n",
        "    print(f\"    EN: {en}\")\n",
        "\n",
        "# Prepare tagged test file for batch eval\n",
        "raw_test_src = Path(\"test.tr\").read_text(encoding=\"utf-8\").splitlines()\n",
        "tagged_test_src = [f\"<nat> {line.strip()}\" for line in raw_test_src]\n",
        "Path(\"test_tagged.tr\").write_text(\"\\n\".join(tagged_test_src), encoding=\"utf-8\")\n",
        "\n",
        "# Translate test set\n",
        "def generate_translations(inputs, batch_size=16):\n",
        "    all_outputs = []\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        batch = inputs[i:i+batch_size]\n",
        "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**encoded, max_length=128)\n",
        "        decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n",
        "        all_outputs.extend(decoded)\n",
        "    return all_outputs\n",
        "\n",
        "# Evaluate predictions\n",
        "pred_lines = generate_translations(tagged_test_src)\n",
        "Path(\"test_pred.en\").write_text(\"\\n\".join(pred_lines), encoding=\"utf-8\")\n",
        "\n",
        "# Calculate BLEU, chrF++, and BERTScore\n",
        "from bert_score import score as bert_score\n",
        "from sacrebleu import corpus_bleu, corpus_chrf\n",
        "import pandas as pd\n",
        "\n",
        "ref_lines = Path(\"test.en\").read_text(encoding=\"utf-8\").splitlines()\n",
        "bleu = corpus_bleu(pred_lines, [ref_lines])\n",
        "chrf = corpus_chrf(pred_lines, [ref_lines])\n",
        "P, R, F1 = bert_score(pred_lines, ref_lines, lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "scores_txt = (\n",
        "    f\"BLEU Score : {bleu.score:.2f}\\n\"\n",
        "    f\"chrF Score : {chrf.score:.2f}\\n\"\n",
        "    f\"BERTScore (F1) Avg : {F1.mean().item():.4f}\\n\"\n",
        ")\n",
        "print(\"Evaluation Metrics:\\n\" + scores_txt)\n",
        "\n",
        "# Save evaluation results\n",
        "output_dir = Path(\"/content/drive/MyDrive/NMT_LMs/Evaluation/\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "df = pd.DataFrame({\n",
        "    \"source_tr\": raw_test_src,\n",
        "    \"reference_en\": ref_lines,\n",
        "    \"prediction_en\": pred_lines,\n",
        "    \"bert_f1\": F1.tolist()\n",
        "})\n",
        "df.to_csv(output_dir / \"translation_results.csv\", index=False, encoding=\"utf-8\")\n",
        "Path(output_dir / \"scores.txt\").write_text(scores_txt, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"All results saved to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "XmmfoMNlex_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the baseline Turkish-English translation model and tokenizer from Hugging Face\n",
        "baseline_model_name = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "baseline_model = MarianMTModel.from_pretrained(baseline_model_name)\n",
        "baseline_tokenizer = MarianTokenizer.from_pretrained(baseline_model_name)\n",
        "\n",
        "# Mount Google Drive to access saved models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths to the fine-tuned and experimental model directories stored in Drive\n",
        "FINETUNED_PATH    = \"/content/drive/MyDrive/fine-tuned-opus-mt-tr-en\"               # Fine-tuned on WMT data\n",
        "NATURAL_PATH      = \"/content/drive/MyDrive/NMT_LMs/Natural_NMT_Model\"             # Fine-tuned with <nat>/<trans> tags\n",
        "LINGUISTIC_PATH   = \"/content/drive/MyDrive/morph-opus-mt2-tr-en\"                   # Fine-tuned with morphological tags (ConcatMorph)\n",
        "\n",
        "# Load all tokenizers and models into memory and set them to evaluation mode (inference only)\n",
        "\n",
        "# Baseline model and tokenizer (no fine-tuning)\n",
        "baseline_tok = MarianTokenizer.from_pretrained(baseline_model_name)\n",
        "baseline_model = MarianMTModel.from_pretrained(baseline_model_name).eval()\n",
        "\n",
        "# Fine-tuned model on WMT parallel data\n",
        "finetuned_tok = MarianTokenizer.from_pretrained(FINETUNED_PATH)\n",
        "finetuned_model = MarianMTModel.from_pretrained(FINETUNED_PATH).eval()\n",
        "\n",
        "# Fine-tuned model trained on natural vs. translationese-tagged data\n",
        "natural_tok = MarianTokenizer.from_pretrained(NATURAL_PATH)\n",
        "natural_model = MarianMTModel.from_pretrained(NATURAL_PATH).eval()\n",
        "\n",
        "# Morphologically-aware fine-tuned model (ConcatMorph input format)\n",
        "ling_tok = MarianTokenizer.from_pretrained(LINGUISTIC_PATH)\n",
        "ling_model = MarianMTModel.from_pretrained(LINGUISTIC_PATH).eval()\n"
      ],
      "metadata": {
        "id": "wTddRSYYe7Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to translate a sentence using a given tokenizer and model\n",
        "def translate(sentence, tokenizer, model):\n",
        "    # Tokenize the input sentence and convert to tensor format\n",
        "    inputs = tokenizer([sentence], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # Disable gradient calculation (inference mode)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "    # Decode the model output into a human-readable string\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Example Turkish sentence to be translated\n",
        "example_tr = \"Ege tavla oynamayı beceremiyor ama Özge hiç beceremiyor.\"\n",
        "print(\"Source Sentence:\", example_tr)\n",
        "\n",
        "# Translate using the baseline pre-trained model (no fine-tuning)\n",
        "print(\"Baseline MT Translation:\")\n",
        "print(translate(example_tr, baseline_tok, baseline_model))\n",
        "\n",
        "# Translate using the WMT fine-tuned model\n",
        "print(\"\\nFinetuned MT Translation:\")\n",
        "print(translate(example_tr, finetuned_tok, finetuned_model))\n",
        "\n",
        "# Translate using NaturalMT model with <nat> and <trans> style tags\n",
        "print(\"\\nNaturalMT Translation:\")\n",
        "print(translate(f\"<nat> {example_tr}\", natural_tok, natural_model))\n",
        "print(translate(f\"<trans>{example_tr}\", natural_tok, natural_model))\n",
        "\n",
        "# Translate a morphologically annotated sentence using the MorphMT model\n",
        "print(\"\\nMorphMT Translation (ConcatMorph format):\")\n",
        "print(translate(\n",
        "    \"Turkiyenin_ekonomiNoun+A3sg+Pnon+Nom son_Adv yılNoun+A3pl+P3sg+Loc büyük_Adj değişiklikNoun+A3pl+P3sg+Acc geçirVerb+Caus+Pos+Past+A3sg+.\",\n",
        "    ling_tok, ling_model\n",
        "))"
      ],
      "metadata": {
        "id": "g6grvymVfEOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyHAT-GJfLKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}